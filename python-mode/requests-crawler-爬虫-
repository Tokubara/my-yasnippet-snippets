# -*- mode: snippet -*-
# name: requests-crawler-爬虫-
# key: 
# --
import requests
import time
from pyquery import PyQuery as pq
import random

#%%
def get_problem_list(difficulty: int):
    difficulty = str(difficulty)
    problems[difficulty] = set()
    headers = {
      'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36'
    }

    for page in range(1,19): # 这个地方有问题
      url=f"https://www.luogu.com.cn/problem/list?page={page}&keyword=&orderBy=&order=&difficulty={difficulty}"
      response=requests.get(url, headers=headers)
      doc = pq(response.text)
      problems[difficulty].update([i.attr("href") for i in doc("li > a").items()])
      time.sleep(random.uniform(0.8,1.5))

get_problem_list(3)
